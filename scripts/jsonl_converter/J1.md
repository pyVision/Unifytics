# Converting JSONL to Parquet: A Comprehensive Technical Guide

Data engineers and analysts frequently work with different file formats to optimize data storage and processing. In this technical guide, we'll explore how to convert JSONL (JSON Lines) files to Apache Parquet format using pandas, and understand why this conversion can significantly improve your data workflow.


## Understanding JSONL and Parquet Formats

JSONL (JSON Lines) format stores each JSON object on a separate line, making it ideal for streaming and processing large datasets. While JSONL offers better streaming capabilities than traditional JSON, converting to Parquet format can provide additional benefits for analytical workloads.


Parquet's columnar storage format offers several advantages:

1. Storage Efficiency
- Column-specific compression reduces storage requirements by up to 75%
- Dictionary encoding for repeated values
- Run-length encoding for sequential data
- More efficient compression compared to line-by-line JSONL storage

2. Query Performance
- Efficient column pruning (reading only required columns)
- Predicate pushdown (filtering at the storage level)
- Reduced I/O operations during data retrieval
- Better performance for analytical queries compared to scanning JSONL lines

3. Schema Evolution
- Add new columns without rewriting existing data
- Drop columns without affecting backward compatibility
- Handle changes in data types effectively

## Converting JSONL to Parquet Using Pandas


Parquet is a columnar storage format that works best with flat, tabular data structures. It doesn't directly support deeply nested JSON objects. Hence if the analytics data contains nested JSON objects we need to convert the same . 

Converting nested JSON to Parquet presents challenges with schema complexity, data types, and structure optimization. Nested structures often require flattening for optimal performance in Parquet's columnar format, as deep nesting can impact query efficiency and memory usage. 
Analytics workloads typically benefit from flatter structures that enable efficient column pruning and better compression ratios. Consider cross-platform compatibility and schema evolution when handling nested structures, as different systems may process them differently.

The converter provides a robust implementation with the following features:
- Chunked processing for memory efficiency
- Nested JSON structure handling
- Performance metrics tracking
- Multiple compression options (snappy, gzip, brotli, none)
- Detailed error handling and logging


## Installation and Setup

### Prerequisites
- Python 3.8 or higher
- Git (for cloning the repository)

### Installation Steps

1. Clone the repository:
```bash
git clone https://github.com/pyVision/Unifytics.git
cd Unifytics/scripts/jsonl_converter
```

2. Install required packages:
```bash
pip install -r requirements.txt
```

The required packages are:
- pandas >= 1.5.0
- pyarrow >= 14.0.1

### Usage Examples:

```bash
# Basic conversion with default settings
python converter.py --input input.jsonl --output output.parquet

# Specify compression algorithm and chunk size
python converter.py --input input.jsonl --output output.parquet --compression gzip --chunk-size 50000

# Use no compression
python converter.py --input input.jsonl --output output.parquet --compression none
```

## Implementation Details

The converter includes several key features to ensure efficient and reliable conversion:

**Performance Metrics**: The converter tracks various performance metrics, including total duration, chunk processing times, compression time, input and output sizes, and compression ratio. These metrics help in monitoring and optimizing the conversion process.

**Memory Efficiency**: To handle large files efficiently, the converter supports configurable chunk sizes and streaming processing. This approach minimizes memory usage and ensures smooth processing of extensive datasets.

**Nested JSON Handling**: The converter can handle nested JSON structures by flattening nested dictionaries and converting lists to strings. It ensures proper column naming to maintain data integrity and compatibility with Parquet's columnar format.

**Robust Error Handling**: Comprehensive logging and exception management are implemented to handle errors effectively. The converter also includes process monitoring to ensure smooth operation and quick identification of issues.

Here's the core conversion function with all these features:

```python
@dataclass
class ConversionMetrics:
    """Stores metrics about the conversion process."""
    total_duration: float = 0
    chunk_times: list = None
    compression_time: float = 0
    input_size: int = 0
    output_size: int = 0
    
    def __post_init__(self):
        self.chunk_times = []

```python
def convert_jsonl_to_parquet(
    input_path: str,
    output_path: str,
    compression: str = 'snappy',
    chunk_size: Optional[int] = 100000
) -> tuple[bool, ConversionMetrics]:
    """
    Convert JSONL file to Parquet format with performance metrics tracking.
    
    Args:
        input_path (str): Path to the input JSONL file
        output_path (str): Path where the output Parquet file will be saved
        compression (str, optional): Compression algorithm to use. Defaults to 'snappy'.
        chunk_size (int, optional): Number of rows to process at once. Defaults to 100000.
    
    Returns:
        tuple[bool, ConversionMetrics]: Success flag and metrics object
    """
    metrics = ConversionMetrics()
    start_time = time.time()
    
    try:
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        # Get the size of the input file
        metrics.input_size = os.path.getsize(input_path)
        # Read the JSONL file in chunks
        reader = pd.read_json(input_path, lines=True, chunksize=chunk_size)
        
        first_chunk = True
        for i, chunk in enumerate(reader):
            chunk_start = time.time()
            compression_start = time.time()
            
            # Handle nested JSON structures
            for column in chunk.select_dtypes(include=['object']):
                if isinstance(chunk[column].iloc[0], dict):
                    # Flatten nested dictionaries
                    nested_df = pd.json_normalize(chunk[column].tolist())
                    nested_df.columns = f"{column}." + nested_df.columns
                    chunk = chunk.drop(columns=[column]).join(nested_df)
                elif isinstance(chunk[column].iloc[0], list):
                    # Convert lists to strings
                    chunk[column] = chunk[column].apply(lambda x: str(x) if x else None)
            
            # Convert the chunk to a Parquet table
            table = pa.Table.from_pandas(chunk)
            
            if first_chunk:
                # Write the first chunk to the Parquet file
                pq.write_table(table, output_path, compression=compression)
                first_chunk = False
            else:
                # Append subsequent chunks to the Parquet file
                pq.write_table(table, output_path, compression=compression, append=True)
            
            # Track compression time
            compression_time = time.time() - compression_start
            metrics.compression_time += compression_time
            
            # Track chunk processing time
            chunk_duration = time.time() - chunk_start
            metrics.chunk_times.append(chunk_duration)
            
            logger.info(f"Chunk {i+1}: Processing time={chunk_duration:.2f}s, "
                       f"Compression time={compression_time:.2f}s")
        
        # Get the size of the output file
        metrics.output_size = os.path.getsize(output_path)
        # Calculate total conversion duration
        metrics.total_duration = time.time() - start_time
        
        logger.info(f"Conversion completed in {metrics.total_duration:.2f}s")
        logger.info(f"Average chunk processing time: "
                   f"{sum(metrics.chunk_times)/len(metrics.chunk_times):.2f}s")
        logger.info(f"Total compression time: {metrics.compression_time:.2f}s")
        logger.info(f"Compression ratio: "
                   f"{metrics.output_size/metrics.input_size:.2%}")
        
        return True, metrics
        
    except Exception as e:
        logger.error(f"Error during conversion: {str(e)}")
        return False, metrics
```



# Performance Monitoring Guidelines

- When converting large JSONL datasets, it's essential to monitor multiple aspects of the conversion process. 
- Track the processing time for individual chunks along with the overall conversion duration and compression time 
to optimize throughput. 
- Keep a close eye on system resources by monitoring memory consumption during processing, 
CPU utilization patterns, and disk I/O performance. 
- After conversion, evaluate the effectiveness by measuring 
the achieved compression ratio, comparing the final file size with the original JSONL, and benchmarking query  performance on the converted Parquet data. These metrics help ensure optimal conversion performance and validate the benefits of the format change.

## Source Code and Contributions

The complete source code is available on GitHub at [https://github.com/pyVision/Unifytics](https://github.com/pyVision/Unifytics). The repository includes:
- Full converter implementation
- Example files
- Documentation

## Conclusion

Converting JSONL to Parquet format using pandas offers significant advantages for analytical workloads. The line-by-line nature of JSONL makes it particularly well-suited for streaming conversion to Parquet, while still benefiting from Parquet's columnar storage advantages.
